{
 "metadata": {
  "name": "",
  "signature": "sha256:1679eb47a8ac7ffa98608dfcdb7e3d4cc2a5dfab67635151c9e8af80cceb1342"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import intelanalytics as ia\n",
      "csvroot = '/user/hadoop/graphflow/'\n",
      "iaroot = '../hadoop/graphflow/'\n",
      "dataset = csvroot + 'groupir.tsv'#'netflow_20140708_ir.tsv'\n",
      "frameset = iaroot + 'groupir.tsv'#'netflow_20140708_ir.tsv'\n",
      "\n",
      "print 'Modules and paths loaded'\n",
      "ia.connect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Modules and paths loaded\n",
        "Already connected to intelanalytics server.\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You only need to run this if you are unsure of what files are in the directory, or what their names are."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ia.drop_graphs('netflow_IR')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -ls $csvroot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 2 items\r\n",
        "-rw-r--r--   3 hadoop hadoop  792387584 2015-02-25 14:03 /user/hadoop/graphflow/groupir.tsv\r\n",
        "-rw-r--r--   3 hadoop hadoop    5652480 2015-02-24 09:16 /user/hadoop/graphflow/netflow_20140708_ir.tsv\r\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creates the IR graph schema."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## sip     dip     sport   dport   firstseen       lastseen        conns   avgbyt  avgpkt  maxbyt  maxpkt  minbyt  minpkt\n",
      "schema = [('srcip', str),('dstip', str),('sport',int),('dport',int)]\n",
      "          #('first_seen', str),('last_seen', str),('conns',long),('avgbyt', float),('avgpkt', float),\n",
      "        #('maxbyt', long),('maxpkt', long),('minbyt', long),('minpkt', long)]\n",
      "csv_file = ia.CsvFile(frameset, schema, delimiter='\\t', skip_header_lines = 1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create data frame (this step can take a while)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Creating DataFrame\")\n",
      "f = ia.Frame(csv_file)\n",
      "#combine with step above#\n",
      "f.add_columns(lambda row: [row.srcip + \"_\" + str(row.sport),row.dstip + \"_\" + str(row.dport)], \n",
      "              [('sip_sport',str), ('dip_dport', str)])\n",
      "print f.inspect(20) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating DataFrame\n",
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR:intelanalytics.core.frame:Traceback (most recent call last):\n",
        "  File \"/usr/lib/python2.7/site-packages/intelanalytics/core/frame.py\", line 1036, in __init__\n",
        "    new_frame_name = self._backend.create(self, source, name, _info)\n",
        "  File \"/usr/lib/python2.7/site-packages/intelanalytics/rest/frame.py\", line 94, in create\n",
        "    self.append(frame, source)\n",
        "  File \"/usr/lib/python2.7/site-packages/intelanalytics/rest/frame.py\", line 302, in append\n",
        "    result = execute_update_frame_command(\"frame:/load\", arguments, frame)\n",
        "  File \"/usr/lib/python2.7/site-packages/intelanalytics/rest/frame.py\", line 722, in execute_update_frame_command\n",
        "    command_info = executor.issue(command_request)\n",
        "  File \"/usr/lib/python2.7/site-packages/intelanalytics/rest/command.py\", line 399, in issue\n",
        "    return self.poll_command_info(response)\n",
        "  File \"/usr/lib/python2.7/site-packages/intelanalytics/rest/command.py\", line 419, in poll_command_info\n",
        "    raise CommandServerError(command_info)\n",
        "CommandServerError: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\n",
        "org.apache.spark.SparkContext.<init>(SparkContext.scala:70)\n",
        "com.intel.intelanalytics.engine.spark.context.SparkContextFactory$class.com$intel$intelanalytics$engine$spark$context$SparkContextFactory$$createContext(SparkContextFactory.scala:74)\n",
        "com.intel.intelanalytics.engine.spark.context.SparkContextFactory$$anonfun$getContext$1.apply(SparkContextFactory.scala:47)\n",
        "com.intel.intelanalytics.engine.spark.context.SparkContextFactory$$anonfun$getContext$1.apply(SparkContextFactory.scala:43)\n",
        "com.intel.event.EventLogging$class.withContext(EventLogging.scala:115)\n",
        "com.intel.intelanalytics.engine.spark.context.SparkContextFactory$.withContext(SparkContextFactory.scala:98)\n",
        "com.intel.intelanalytics.engine.spark.context.SparkContextFactory$class.getContext(SparkContextFactory.scala:42)\n",
        "com.intel.intelanalytics.engine.spark.context.SparkContextFactory$.getContext(SparkContextFactory.scala:98)\n",
        "com.intel.intelanalytics.engine.spark.context.SparkContextFactory$class.context(SparkContextFactory.scala:55)\n",
        "com.intel.intelanalytics.engine.spark.context.SparkContextFactory$.context(SparkContextFactory.scala:98)\n",
        "com.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$createSparkContextForCommand$1.apply(CommandExecutor.scala:359)\n",
        "com.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$createSparkContextForCommand$1.apply(CommandExecutor.scala:356)\n",
        "com.intel.intelanalytics.component.ClassLoaderAware$class.withMyClassLoader(ClassLoaderAware.scala:52)\n",
        "com.intel.intelanalytics.engine.spark.command.CommandExecutor.withMyClassLoader(CommandExecutor.scala:105)\n",
        "com.intel.intelanalytics.engine.spark.command.CommandExecutor.createSparkContextForCommand(CommandExecutor.scala:356)\n",
        "com.intel.intelanalytics.engine.spark.command.CommandExecutor.com$intel$intelanalytics$engine$spark$command$CommandExecutor$$getInvocation(CommandExecutor.scala:304)\n",
        "com.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$com$intel$intelanalytics$engine$spark$command$CommandExecutor$$executeCommandContext$1.apply(CommandExecutor.scala:213)\n",
        "com.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$com$intel$intelanalytics$engine$spark$command$CommandExecutor$$executeCommandContext$1.apply(CommandExecutor.scala:209)\n",
        "com.intel.event.EventLogging$class.withContext(EventLogging.scala:115)\n",
        "com.intel.intelanalytics.engine.spark.command.CommandExecutor.withContext(CommandExecutor.scala:105) (command: 121, corId: 4b41270b-4c01-4632-9c94-3430f1530eec)\n",
        "\n"
       ]
      },
      {
       "ename": "CommandServerError",
       "evalue": "Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\norg.apache.spark.SparkContext.<init>(SparkContext.scala:70)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$class.com$intel$intelanalytics$engine$spark$context$SparkContextFactory$$createContext(SparkContextFactory.scala:74)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$$anonfun$getContext$1.apply(SparkContextFactory.scala:47)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$$anonfun$getContext$1.apply(SparkContextFactory.scala:43)\ncom.intel.event.EventLogging$class.withContext(EventLogging.scala:115)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$.withContext(SparkContextFactory.scala:98)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$class.getContext(SparkContextFactory.scala:42)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$.getContext(SparkContextFactory.scala:98)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$class.context(SparkContextFactory.scala:55)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$.context(SparkContextFactory.scala:98)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$createSparkContextForCommand$1.apply(CommandExecutor.scala:359)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$createSparkContextForCommand$1.apply(CommandExecutor.scala:356)\ncom.intel.intelanalytics.component.ClassLoaderAware$class.withMyClassLoader(ClassLoaderAware.scala:52)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor.withMyClassLoader(CommandExecutor.scala:105)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor.createSparkContextForCommand(CommandExecutor.scala:356)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor.com$intel$intelanalytics$engine$spark$command$CommandExecutor$$getInvocation(CommandExecutor.scala:304)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$com$intel$intelanalytics$engine$spark$command$CommandExecutor$$executeCommandContext$1.apply(CommandExecutor.scala:213)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$com$intel$intelanalytics$engine$spark$command$CommandExecutor$$executeCommandContext$1.apply(CommandExecutor.scala:209)\ncom.intel.event.EventLogging$class.withContext(EventLogging.scala:115)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor.withContext(CommandExecutor.scala:105) (command: 121, corId: 4b41270b-4c01-4632-9c94-3430f1530eec)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mCommandServerError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-75-b4484b2e2086>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating DataFrame\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#combine with step above#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m f.add_columns(lambda row: [row.srcip + \"_\" + str(row.sport),row.dstip + \"_\" + str(row.dport)], \n\u001b[0;32m      5\u001b[0m               [('sip_sport',str), ('dip_dport', str)])\n",
        "\u001b[1;32m/usr/lib/python2.7/site-packages/intelanalytics/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, source, name, _info)\u001b[0m\n\u001b[0;32m   1035\u001b[0m             \u001b[0m_BaseFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[0mnew_frame_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Created new frame \"%s\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_frame_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/site-packages/intelanalytics/meta/api.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m  \u001b[1;31m# see intelanalytics.errors.last for details\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mCommandServerError\u001b[0m: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\norg.apache.spark.SparkContext.<init>(SparkContext.scala:70)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$class.com$intel$intelanalytics$engine$spark$context$SparkContextFactory$$createContext(SparkContextFactory.scala:74)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$$anonfun$getContext$1.apply(SparkContextFactory.scala:47)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$$anonfun$getContext$1.apply(SparkContextFactory.scala:43)\ncom.intel.event.EventLogging$class.withContext(EventLogging.scala:115)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$.withContext(SparkContextFactory.scala:98)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$class.getContext(SparkContextFactory.scala:42)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$.getContext(SparkContextFactory.scala:98)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$class.context(SparkContextFactory.scala:55)\ncom.intel.intelanalytics.engine.spark.context.SparkContextFactory$.context(SparkContextFactory.scala:98)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$createSparkContextForCommand$1.apply(CommandExecutor.scala:359)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$createSparkContextForCommand$1.apply(CommandExecutor.scala:356)\ncom.intel.intelanalytics.component.ClassLoaderAware$class.withMyClassLoader(ClassLoaderAware.scala:52)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor.withMyClassLoader(CommandExecutor.scala:105)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor.createSparkContextForCommand(CommandExecutor.scala:356)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor.com$intel$intelanalytics$engine$spark$command$CommandExecutor$$getInvocation(CommandExecutor.scala:304)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$com$intel$intelanalytics$engine$spark$command$CommandExecutor$$executeCommandContext$1.apply(CommandExecutor.scala:213)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor$$anonfun$com$intel$intelanalytics$engine$spark$command$CommandExecutor$$executeCommandContext$1.apply(CommandExecutor.scala:209)\ncom.intel.event.EventLogging$class.withContext(EventLogging.scala:115)\ncom.intel.intelanalytics.engine.spark.command.CommandExecutor.withContext(CommandExecutor.scala:105) (command: 121, corId: 4b41270b-4c01-4632-9c94-3430f1530eec)"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create nodes - there will be two kinds "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sips =  ia.VertexRule(\"ip\", f.srcip,{\"vertex_type\": \"C\"})\n",
      "dips =  ia.VertexRule(\"ip\", f.dstip,{\"vertex_type\": \"C\"})\n",
      "sports = ia.VertexRule(\"src\", f.sip_sport,{\"vertex_type\": \"L\"})\n",
      "dports = ia.VertexRule(\"dst\", f.dip_dport,{\"vertex_type\": \"R\"})\n",
      "fromport = ia.EdgeRule(\"sport\", sips,sports,{\"fport\":f.sport},bidirectional=False)\n",
      "toport = ia.EdgeRule(\"dport\", dips,dports,{\"tport\":f.dport},bidirectional=False)\n",
      "conn = ia.EdgeRule(\"connect\", sports, dports, {\"conns\":f.conns,\"avg_byte\": f.avgbyt, \"avg_pkt\": f.avgpkt,\n",
      "\"max_byte\": f.maxbyt, \"max_pkt\": f.maxpkt,\"min_byte\": f.minbyt, \"max_pkt\": f.minpkt},bidirectional=False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gname = 'netflow_IR'\n",
      "\n",
      "print(\"Creating \" + gname)\n",
      "\n",
      "g = ia.TitanGraph([sports,dports,conn,sips,sports,fromport,dips,dports,toport] ,gname)\n",
      "print(\"graph task completed\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating netflow_IR\n",
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "initializing..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:00"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:02"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:04"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[.........................]   0.56% Tasks retries:0 Time 0:00:22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[======...................]  25.56% Tasks retries:0 Time 0:00:24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[=======..................]  31.11% Tasks retries:0 Time 0:00:26"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[=======..................]  31.11% Tasks retries:0 Time 0:00:28"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[=======..................]  31.11% Tasks retries:0 Time 0:00:30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[=======..................]  31.11% Tasks retries:0 Time 0:00:32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[=======..................]  31.11% Tasks retries:0 Time 0:00:34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[========.................]  33.88% Tasks retries:0 Time 0:00:36"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[=========................]  36.67% Tasks retries:0 Time 0:00:38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[==========...............]  42.78% Tasks retries:0 Time 0:00:41"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[==================.......]  75.18% Tasks retries:0 Time 0:00:43"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[==================.......]  75.18% Tasks retries:0 Time 0:00:45"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[==================.......]  75.18% Tasks retries:0 Time 0:00:47"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[==================.......]  75.18% Tasks retries:0 Time 0:00:50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[==================.......]  75.18% Tasks retries:0 Time 0:00:53"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[==================.......]  75.18% Tasks retries:0 Time 0:00:56"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[==================.......]  75.18% Tasks retries:0 Time 0:00:59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[==================.......]  75.18% Tasks retries:0 Time 0:01:02"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[==================.......]  75.18% Tasks retries:0 Time 0:01:05"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[======================...]  88.70% Tasks retries:0 Time 0:01:08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "[=========================] 100.00% Tasks retries:0 Time 0:01:11\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "graph task completed\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}